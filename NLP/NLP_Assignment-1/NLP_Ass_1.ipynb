{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment No.1:** Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library. Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization"
      ],
      "metadata": {
        "id": "7HBQnOK4Ngob"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxXKx6ZwskOP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, regexp_tokenize, TweetTokenizer, WhitespaceTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zn26hyEsxB2",
        "outputId": "df8cfaee-5d94-429d-9c8d-b8a4127626ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Data\n"
      ],
      "metadata": {
        "id": "XWwzb89SIgyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Running is fun! Let's go for a run... excited\""
      ],
      "metadata": {
        "id": "23QZRoZNtZqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Whitespace Tokenizer**:A whitespace tokenizer is a simple text tokenization method where the input text is split into tokens (words or phrases) based on whitespace (spaces, tabs, newlines)"
      ],
      "metadata": {
        "id": "qHCwde61IlXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
        "print(\"Whitespace Tokens:\", whitespace_tokens)\n"
      ],
      "metadata": {
        "id": "V-5CmANCtoeN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe94a57d-e25a-44d8-c12b-53cc8622cff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokens: ['Running', 'is', 'fun!', \"Let's\", 'go', 'for', 'a', 'run...', 'excited']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**punctuation tokenizer**:A punctuation tokenizer splits text based on punctuation marks (such as periods, commas, exclamation marks, etc.)"
      ],
      "metadata": {
        "id": "kf2i44NyJOLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def punctuation_tokenizer(text):\n",
        "    return re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "\n",
        "text = \"Hello, world! How's it going?\"\n",
        "tokens = punctuation_tokenizer(text)\n",
        "\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "e9QrTl6EtqZm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eabcbb6-5b3d-425e-d253-4cce6c23eed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!', 'How', \"'\", 's', 'it', 'going', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "print(\"Treebank Tokens:\", treebank_tokens)\n"
      ],
      "metadata": {
        "id": "vRVnqB9dtsf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1bc2ed0-9ba6-4bb2-be06-5ed2f6178dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokens: ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'going', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"Tweet Tokens:\", tweet_tokens)"
      ],
      "metadata": {
        "id": "Jjx3wvJptu0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae71592-b423-4236-cd46-d814884e1e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokens: ['Hello', ',', 'world', '!', \"How's\", 'it', 'going', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "snowball_stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "trDVXGJWNS3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stems = [porter_stemmer.stem(word) for word in treebank_tokens]\n",
        "snowball_stems = [snowball_stemmer.stem(word) for word in treebank_tokens]\n",
        "print(\"Snowball Stemmed Tokens:\", snowball_stems)\n"
      ],
      "metadata": {
        "id": "iZn0RG0pt02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03889cd-3f13-4914-9024-7fa6287016ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemmed Tokens: ['hello', ',', 'world', '!', 'how', \"'s\", 'it', 'go', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "U475mglht3qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in treebank_tokens]\n",
        "print(\"Lemmatized Tokens (verbs):\", lemmatized_words)\n"
      ],
      "metadata": {
        "id": "dZnd5kqPt7UN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "974709b1-3719-4a87-de1b-6bbc42d40de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens (verbs): ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'go', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Whitespace Tokens:\", whitespace_tokens)\n",
        "print(\"Treebank Tokens:\", treebank_tokens)\n",
        "print(\"Tweet Tokens:\", tweet_tokens)\n",
        "print(\"Porter Stemmed Tokens:\", porter_stems)\n",
        "print(\"Snowball Stemmed Tokens:\", snowball_stems)\n",
        "print(\"Lemmatized Tokens (verbs):\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xbMQFLxt-bt",
        "outputId": "51c9fe01-1f21-4b82-f8da-694feef1f20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokens: ['Running', 'is', 'fun!', \"Let's\", 'go', 'for', 'a', 'run...', 'excited']\n",
            "Treebank Tokens: ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'going', '?']\n",
            "Tweet Tokens: ['Hello', ',', 'world', '!', \"How's\", 'it', 'going', '?']\n",
            "Porter Stemmed Tokens: ['hello', ',', 'world', '!', 'how', \"'s\", 'it', 'go', '?']\n",
            "Snowball Stemmed Tokens: ['hello', ',', 'world', '!', 'how', \"'s\", 'it', 'go', '?']\n",
            "Lemmatized Tokens (verbs): ['Hello', ',', 'world', '!', 'How', \"'s\", 'it', 'go', '?']\n"
          ]
        }
      ]
    }
  ]
}